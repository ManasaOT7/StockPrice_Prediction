# -*- coding: utf-8 -*-
"""WISEPROJECT_DECISION_TREE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BEORcb-O2p3IGBHfgToyehueYL3igs1J
"""

!pip install opendatasets
!pip install pandas
!pip install kaggle

import pandas as pd
import zipfile
import os

os.environ['KAGGLE_USERNAME'] = "konayamini"
os.environ['KAGGLE_KEY'] = "2ad71286b832cf829aea927a6b59095d"

#!mkdir ~/.kaggle
kaggle_dir = '/root/.kaggle'
if not os.path.exists(kaggle_dir):
    os.makedirs(kaggle_dir)
!echo '{"username":"konayamini","key":"2ad71286b832cf829aea927a6b59095d"}' > /root/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d dgawlik/nyse -f prices.csv --force

with zipfile.ZipFile('prices.csv.zip', 'r') as zip_ref:
    zip_ref.extractall('nyse_dataset')
dataset = pd.read_csv('nyse_dataset/prices.csv')
print(dataset.head())

missing_values_count = dataset.isnull().sum()

# Display the count of missing values for each column
print("Missing Values Count:")
print(missing_values_count)

import numpy as np
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
import pandas as pd
from sklearn.metrics import r2_score

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')

columns_to_impute = ['open', 'close', 'low', 'high', 'volume']
dataset[columns_to_impute] = imputer.fit_transform(dataset[columns_to_impute])

numeric_columns = ['open', 'close', 'low', 'high', 'volume']

Q1 = dataset[numeric_columns].quantile(0.25)
Q3 = dataset[numeric_columns].quantile(0.75)
IQR = Q3 - Q1

outliers_mask = ((dataset[numeric_columns] < (Q1 - 1.5 * IQR)) | (dataset[numeric_columns] > (Q3 + 1.5 * IQR)))

outliers_count = outliers_mask.sum()

print(outliers_count)

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

for column in numeric_columns:
    dataset[column] = dataset[column].clip(lower=lower_bound[column], upper=upper_bound[column])

outliers_mask = ((dataset[numeric_columns] < (Q1 - 1.5 * IQR)) | (dataset[numeric_columns] > (Q3 + 1.5 * IQR)))
outliers_count = outliers_mask.sum()
print(outliers_count)

# Scale features
X = dataset[['open', 'high', 'low', 'volume']]
y = dataset['close']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train[['open', 'high', 'low', 'volume']])
X_test_scaled = scaler.transform(X_test[['open', 'high', 'low', 'volume']])

# Create DecisionTreeRegressor model
dt_regressor = DecisionTreeRegressor(random_state=42)  # You can adjust parameters as needed
dt_regressor.fit(X_train_scaled, y_train)

y_class = np.where(y > y.median(), 1, 0)  # Convert to binary classification
X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X, y_class, test_size=0.2, random_state=42)

# Create DecisionTreeClassifier model
dt_classifier = DecisionTreeClassifier()  # You can adjust parameters as needed
dt_classifier.fit(X_train_class, y_train_class)

# Predict using Decision Tree models
y_pred_dt_regressor = dt_regressor.predict(X_test_scaled)
mae_dt = metrics.mean_absolute_error(y_test, y_pred_dt_regressor)
mse_dt = metrics.mean_squared_error(y_test, y_pred_dt_regressor)
print(f'Decision Tree Regressor - Mean Absolute Error (MAE): {mae_dt}')
print(f'Decision Tree Regressor - Mean Squared Error (MSE): {mse_dt}')

# Calculate R-squared (coefficient of determination)
r2_dt = r2_score(y_test, y_pred_dt_regressor)
print(f'Decision Tree Regressor - R-squared (Coefficient of Determination): {r2_dt}')

y_pred_dt_classifier = dt_classifier.predict(X_test_class)
accuracy_dt_cls = metrics.accuracy_score(y_test_class, y_pred_dt_classifier)
precision_dt_cls = metrics.precision_score(y_test_class, y_pred_dt_classifier)
recall_dt_cls = metrics.recall_score(y_test_class, y_pred_dt_classifier)
f1_dt_cls = metrics.f1_score(y_test_class, y_pred_dt_classifier)

print(f'Decision Tree Classifier - Accuracy: {accuracy_dt_cls}')
print(f'Decision Tree Classifier - Precision: {precision_dt_cls}')
print(f'Decision Tree Classifier - Recall: {recall_dt_cls}')
print(f'Decision Tree Classifier - F1-Score: {f1_dt_cls}')

# Predict using Decision Tree Classifier model on the training dataset
y_train_pred_dt_classifier = dt_classifier.predict(X_train_class)
accuracy_dt_cls_train = metrics.accuracy_score(y_train_class, y_train_pred_dt_classifier)
precision_dt_cls_train = metrics.precision_score(y_train_class, y_train_pred_dt_classifier)
recall_dt_cls_train = metrics.recall_score(y_train_class, y_train_pred_dt_classifier)
f1_dt_cls_train = metrics.f1_score(y_train_class, y_train_pred_dt_classifier)

# Print out the performance metrics for the training dataset
print(f'Decision Tree Classifier - Training Accuracy: {accuracy_dt_cls_train}')
print(f'Decision Tree Classifier - Training Precision: {precision_dt_cls_train}')
print(f'Decision Tree Classifier - Training Recall: {recall_dt_cls_train}')
print(f'Decision Tree Classifier - Training F1-Score: {f1_dt_cls_train}')

# User input for Decision Tree prediction
while True:
    open_val = float(input("Enter open value: "))
    low_val = float(input("Enter low value: "))
    high_val = float(input("Enter high value: "))
    volume_val = float(input("Enter volume value: "))

    user_input = [[open_val, low_val, high_val, volume_val]]
    user_input_df = pd.DataFrame(user_input, columns=['open', 'low', 'high', 'volume'])

    # Select columns in the same order as during fitting
    user_input_scaled = scaler.transform(user_input_df[['open', 'high', 'low', 'volume']])

    predicted_close_dt_regressor = dt_regressor.predict(user_input_scaled)
    print("Decision Tree Regressor - Predicted close value:", predicted_close_dt_regressor[0])

    predicted_class_dt = dt_classifier.predict(user_input_df[['open', 'high', 'low', 'volume']])
    print("Decision Tree Classifier - Predicted class:", predicted_class_dt[0])

    choice = input("Do you want to make another prediction? (y/n): ")
    if choice.lower() != 'y':
        break

# Evaluate Decision Tree Regressor
train_score_regressor = dt_regressor.score(X_train_scaled, y_train)
test_score_regressor = dt_regressor.score(X_test_scaled, y_test)

# Evaluate Decision Tree Classifier
train_score_classifier = dt_classifier.score(X_train_class, y_train_class)
test_score_classifier = dt_classifier.score(X_test_class, y_test_class)

print("Decision Tree Regressor:")
print("Training R2 Score:", r2_score(y_train, dt_regressor.predict(X_train_scaled)))
print("Testing R2 Score:", r2_score(y_test, y_pred_dt_regressor))
print("\nDecision Tree Classifier:")
print("Training Accuracy Score:", train_score_classifier)
print("Testing Accuracy Score:", test_score_classifier)

# Check for overfitting or underfitting
if train_score_regressor < test_score_regressor:
    print("Warning: Decision Tree Regressor may be overfitting.")
elif train_score_regressor > test_score_regressor:
    print("Warning: Decision Tree Regressor may be underfitting.")
else:
    print("Decision Tree Regressor is performing reasonably well.")

if train_score_classifier < test_score_classifier:
    print("Warning: Decision Tree Classifier may be overfitting.")
elif train_score_classifier > test_score_classifier:
    print("Warning: Decision Tree Classifier may be underfitting.")
else:
    print("Decision Tree Classifier is performing reasonably well.")



